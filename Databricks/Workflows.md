
- **Workflows** is a fully-managed cloud-based general-purpose task orchestration service for the entire [[Lakehouse]].
- **Workflows** is a service for data engineers, data scientists and analysts to build reliable data, analytics and Al workflows on any cloud.
- Databricks workflows enables all data teams to orchestrate any combination of tasks such as notebooks, SQL, ML Models and Python code.
- As workflows sit on the top of the Databricks platform, it can be fully integrated with other services
- With Databricks workflows, your team can easily create, run , monitor and repair data pipelines without managing any infrastructure.

### Use cases

- **Orchestration of Dependent jobs** - Jobs running on schedule, containing dependent tasks/steps
- **Machine Learning Tasks** - Run MLfIow notebook task in a job
- **Arbitrary Code, External API Calls, Custom Tasks** - Run tasks in a job which an contain Jar file, Spark Submit, Python Script, SQL task, dbt.

>[[Delta Live Tables (DLT)]] pipeline can be a task in a workflow

### How to leverage workflows

- Allows you to build simple ETL/ML task orchestration
- Reduces infrastructure overhead
- Easily integrate with external tools
- Enables non-engineers to build their own workflows using simple Ul
- Cloud-provider independent
- Enables re-using clusters to reduce cost and startup time

